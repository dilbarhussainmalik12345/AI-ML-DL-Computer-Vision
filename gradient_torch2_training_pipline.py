# -*- coding: utf-8 -*-
"""Gradient_torch2_training_pipline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zAa6tvyckwY0-aOV5RiD9mWtWbdtIT5n
"""

# 1.Design Model(input, outputsize, forward pass)
# 2. Construct Loss and Optimizer
# 3. Training Loop
#   - Forwards pass: ComputePrediction
#   - Backwards Pass: Gradient
#  - update weights

import torch
import torch.nn as nn

# f = 2*x
X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)
Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)

X_test = torch.tensor([5], dtype = torch.float32)

n_samples, n_features = X.shape
print(n_samples, n_features)

input_size = n_features
output_size = n_features
#model = nn.Linear(input_size, output_size)

#For Custom Model
class LinearRegression(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegression, self).__init__()
        #define Layers
        self.lin = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.lin(x)

model = LinearRegression(input_size, output_size)

print(f'Prediction Before Training: f(5)= {model(X_test).item():.3f}')


  #Training
learning_rate = 0.01
n_iters = 100

loss = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)


for epoch in range(n_iters):
  #prediction  = forward pass
  ypred = model(X)

  #loss
  l = loss(Y, ypred)

  #gradients = backward pass
  l.backward()

  #Update the weights
  optimizer.step()

#Zero gradients
  optimizer.zero_grad()

  if epoch % 10 == 0:
    [w, b] = model.parameters()
    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')

print(f'Prediction After Training: f(5)= {model(X_test).item():.3f}')

